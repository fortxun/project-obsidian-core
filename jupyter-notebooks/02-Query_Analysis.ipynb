{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis (QAN)\n",
    "\n",
    "This notebook demonstrates how to analyze database query performance using data collected by Project Obsidian Core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Connection to Druid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Connect to Druid SQL\n",
    "druid_url = \"druid://druid-broker:8082/druid/v2/sql/\"\n",
    "engine = create_engine(druid_url)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\"SELECT 1 AS test\").fetchone()\n",
    "    print(f\"Connected to Druid successfully: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Druid: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for QAN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_top_queries(db_system, metric_column, time_window_minutes=60, limit=10):\n",
    "    \"\"\"\n",
    "    Get top queries based on a specific metric for a given database system and time window.\n",
    "    \n",
    "    Parameters:\n",
    "    - db_system: 'mysql' or 'postgresql'\n",
    "    - metric_column: Column to sort by (e.g., 'sum_total_exec_time', 'sum_rows_examined')\n",
    "    - time_window_minutes: Time window in minutes (default: 60)\n",
    "    - limit: Number of queries to return (default: 10)\n",
    "    \"\"\"\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(minutes=time_window_minutes)\n",
    "    \n",
    "    # Determine digest/id column based on db_system\n",
    "    id_column = '\"db.statement.digest\"' if db_system == 'mysql' else '\"db.query.id\"'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "      {id_column} AS query_id,\n",
    "      \"db.statement.sample\" AS query_sample,\n",
    "      \"resource.instance.id\" AS instance_id,\n",
    "      SUM({metric_column}) AS total,\n",
    "      SUM(sum_calls) AS calls,\n",
    "      CASE WHEN SUM(sum_calls) > 0 THEN SUM({metric_column}) / SUM(sum_calls) ELSE 0 END AS avg_per_call\n",
    "    FROM qan_db\n",
    "    WHERE \n",
    "      \"db.system\" = '{db_system}' AND\n",
    "      __time BETWEEN TIMESTAMP '{start_time.strftime('%Y-%m-%d %H:%M:%S')}' AND TIMESTAMP '{end_time.strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "    GROUP BY 1, 2, 3\n",
    "    ORDER BY 4 DESC\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql(query, engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching top queries: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def format_sample(sample, max_length=80):\n",
    "    \"\"\"Format query sample for display\"\"\"\n",
    "    if not sample or pd.isna(sample):\n",
    "        return \"[No sample available]\"\n",
    "        \n",
    "    sample = str(sample).strip()\n",
    "    if len(sample) > max_length:\n",
    "        return sample[:max_length] + \"...\"\n",
    "    return sample\n",
    "\n",
    "def analyze_query_trend(db_system, query_id, metric_column, time_window_hours=24, interval_minutes=5):\n",
    "    \"\"\"\n",
    "    Analyze trend of a specific query over time.\n",
    "    \n",
    "    Parameters:\n",
    "    - db_system: 'mysql' or 'postgresql'\n",
    "    - query_id: The digest or ID of the query\n",
    "    - metric_column: Column to analyze (e.g., 'sum_total_exec_time', 'sum_rows_examined')\n",
    "    - time_window_hours: Time window in hours (default: 24)\n",
    "    - interval_minutes: Interval for time buckets in minutes (default: 5)\n",
    "    \"\"\"\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(hours=time_window_hours)\n",
    "    \n",
    "    # Determine id column based on db_system\n",
    "    id_column = '\"db.statement.digest\"' if db_system == 'mysql' else '\"db.query.id\"'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "      time_floor(__time, 'PT{interval_minutes}M') AS time_bucket,\n",
    "      SUM({metric_column}) AS total,\n",
    "      SUM(sum_calls) AS calls,\n",
    "      CASE WHEN SUM(sum_calls) > 0 THEN SUM({metric_column}) / SUM(sum_calls) ELSE 0 END AS avg_per_call\n",
    "    FROM qan_db\n",
    "    WHERE \n",
    "      \"db.system\" = '{db_system}' AND\n",
    "      {id_column} = '{query_id}' AND\n",
    "      __time BETWEEN TIMESTAMP '{start_time.strftime('%Y-%m-%d %H:%M:%S')}' AND TIMESTAMP '{end_time.strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql(query, engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching query trend: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 MySQL Queries by Execution Time (Last Hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get top 10 MySQL queries by execution time in the last hour\n",
    "mysql_top_queries = get_top_queries('mysql', 'sum_total_exec_time', time_window_minutes=60, limit=10)\n",
    "\n",
    "if not mysql_top_queries.empty:\n",
    "    # Format for display\n",
    "    display_df = mysql_top_queries.copy()\n",
    "    display_df['query_sample'] = display_df['query_sample'].apply(format_sample)\n",
    "    display_df['total_seconds'] = display_df['total'] / 1000000000  # Convert from nanoseconds to seconds\n",
    "    display_df['avg_seconds'] = display_df['avg_per_call'] / 1000000000  # Convert from nanoseconds to seconds\n",
    "    \n",
    "    # Display table\n",
    "    display(display_df[['query_sample', 'calls', 'total_seconds', 'avg_seconds', 'instance_id']].rename(\n",
    "        columns={\n",
    "            'query_sample': 'Query',\n",
    "            'calls': 'Executions',\n",
    "            'total_seconds': 'Total Time (s)',\n",
    "            'avg_seconds': 'Avg Time (s)',\n",
    "            'instance_id': 'Instance'\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig = px.bar(\n",
    "        display_df, \n",
    "        x='query_id', \n",
    "        y='total_seconds',\n",
    "        hover_data=['query_sample', 'calls', 'avg_seconds'],\n",
    "        labels={\n",
    "            'query_id': 'Query ID',\n",
    "            'total_seconds': 'Total Execution Time (s)',\n",
    "            'query_sample': 'Query',\n",
    "            'calls': 'Executions',\n",
    "            'avg_seconds': 'Avg Time (s)'\n",
    "        },\n",
    "        title='Top 10 MySQL Queries by Execution Time (Last Hour)'\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No MySQL query data found for the last hour.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 PostgreSQL Queries by Rows Examined (Last Hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get top 10 PostgreSQL queries by rows examined in the last hour\n",
    "postgres_top_queries = get_top_queries('postgresql', 'sum_rows_examined', time_window_minutes=60, limit=10)\n",
    "\n",
    "if not postgres_top_queries.empty:\n",
    "    # Format for display\n",
    "    display_df = postgres_top_queries.copy()\n",
    "    display_df['query_sample'] = display_df['query_sample'].apply(format_sample)\n",
    "    \n",
    "    # Display table\n",
    "    display(display_df[['query_sample', 'calls', 'total', 'avg_per_call', 'instance_id']].rename(\n",
    "        columns={\n",
    "            'query_sample': 'Query',\n",
    "            'calls': 'Executions',\n",
    "            'total': 'Total Rows Examined',\n",
    "            'avg_per_call': 'Avg Rows per Call',\n",
    "            'instance_id': 'Instance'\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig = px.bar(\n",
    "        display_df, \n",
    "        x='query_id', \n",
    "        y='total',\n",
    "        hover_data=['query_sample', 'calls', 'avg_per_call'],\n",
    "        labels={\n",
    "            'query_id': 'Query ID',\n",
    "            'total': 'Total Rows Examined',\n",
    "            'query_sample': 'Query',\n",
    "            'calls': 'Executions',\n",
    "            'avg_per_call': 'Avg Rows per Call'\n",
    "        },\n",
    "        title='Top 10 PostgreSQL Queries by Rows Examined (Last Hour)'\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No PostgreSQL query data found for the last hour.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Trend Analysis for Specific Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select one of the top queries for trend analysis\n",
    "if not mysql_top_queries.empty:\n",
    "    # Get the first query ID from top queries\n",
    "    selected_query_id = mysql_top_queries.iloc[0]['query_id']\n",
    "    selected_query_sample = mysql_top_queries.iloc[0]['query_sample']\n",
    "    \n",
    "    print(f\"Analyzing trend for query: {format_sample(selected_query_sample)}\")\n",
    "    \n",
    "    # Get trend data for the last 24 hours with 5-minute intervals\n",
    "    trend_df = analyze_query_trend('mysql', selected_query_id, 'sum_total_exec_time', time_window_hours=24, interval_minutes=5)\n",
    "    \n",
    "    if not trend_df.empty:\n",
    "        # Convert time values to seconds\n",
    "        trend_df['total_seconds'] = trend_df['total'] / 1000000000\n",
    "        trend_df['avg_seconds'] = trend_df['avg_per_call'] / 1000000000\n",
    "        \n",
    "        # Create line chart for execution time trend\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=trend_df['time_bucket'],\n",
    "            y=trend_df['total_seconds'],\n",
    "            mode='lines',\n",
    "            name='Total Time (s)'\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=trend_df['time_bucket'],\n",
    "            y=trend_df['calls'],\n",
    "            mode='lines',\n",
    "            name='Executions',\n",
    "            yaxis='y2'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"Query Execution Trend (Last 24 Hours)\",\n",
    "            xaxis_title=\"Time\",\n",
    "            yaxis_title=\"Total Time (s)\",\n",
    "            yaxis2=dict(\n",
    "                title=\"Executions\",\n",
    "                overlaying=\"y\",\n",
    "                side=\"right\"\n",
    "            ),\n",
    "            height=500,\n",
    "            legend=dict(x=0.01, y=0.99, orientation='h')\n",
    "        )\n",
    "        fig.show()\n",
    "        \n",
    "        # Create line chart for average execution time trend\n",
    "        fig2 = px.line(\n",
    "            trend_df, \n",
    "            x=\"time_bucket\", \n",
    "            y=\"avg_seconds\",\n",
    "            title=\"Average Execution Time per Call (Last 24 Hours)\",\n",
    "            labels={\n",
    "                \"time_bucket\": \"Time\",\n",
    "                \"avg_seconds\": \"Avg Time per Call (s)\"\n",
    "            }\n",
    "        )\n",
    "        fig2.update_layout(height=400)\n",
    "        fig2.show()\n",
    "    else:\n",
    "        print(\"No trend data found for the selected query.\")\n",
    "else:\n",
    "    print(\"No queries available for trend analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Query Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to get multiple top metrics\n",
    "def get_top_by_multiple_metrics(db_system='mysql', time_window_minutes=60, limit=10):\n",
    "    end_time = datetime.now()\n",
    "    start_time = end_time - timedelta(minutes=time_window_minutes)\n",
    "    \n",
    "    # Determine id column based on db_system\n",
    "    id_column = '\"db.statement.digest\"' if db_system == 'mysql' else '\"db.query.id\"'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH aggregated AS (\n",
    "      SELECT \n",
    "        {id_column} AS query_id,\n",
    "        \"db.statement.sample\" AS query_sample,\n",
    "        SUM(sum_total_exec_time) AS exec_time,\n",
    "        SUM(sum_rows_examined) AS rows_examined,\n",
    "        SUM(sum_calls) AS calls,\n",
    "        SUM(COALESCE(sum_temp_disk_tables, 0)) AS temp_disk_tables\n",
    "      FROM qan_db\n",
    "      WHERE \n",
    "        \"db.system\" = '{db_system}' AND\n",
    "        __time BETWEEN TIMESTAMP '{start_time.strftime('%Y-%m-%d %H:%M:%S')}' AND TIMESTAMP '{end_time.strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "      GROUP BY 1, 2\n",
    "    ),\n",
    "    exec_time_rank AS (\n",
    "      SELECT query_id, query_sample, exec_time, calls, rows_examined, temp_disk_tables,\n",
    "             ROW_NUMBER() OVER (ORDER BY exec_time DESC) AS rank\n",
    "      FROM aggregated\n",
    "    ),\n",
    "    rows_rank AS (\n",
    "      SELECT query_id, query_sample, exec_time, calls, rows_examined, temp_disk_tables,\n",
    "             ROW_NUMBER() OVER (ORDER BY rows_examined DESC) AS rank\n",
    "      FROM aggregated\n",
    "    ),\n",
    "    temp_tables_rank AS (\n",
    "      SELECT query_id, query_sample, exec_time, calls, rows_examined, temp_disk_tables,\n",
    "             ROW_NUMBER() OVER (ORDER BY temp_disk_tables DESC) AS rank\n",
    "      FROM aggregated\n",
    "      WHERE temp_disk_tables > 0\n",
    "    )\n",
    "    SELECT 'Execution Time' AS metric, query_id, query_sample, exec_time AS value, calls, rows_examined, temp_disk_tables\n",
    "    FROM exec_time_rank\n",
    "    WHERE rank <= {limit}\n",
    "    UNION ALL\n",
    "    SELECT 'Rows Examined' AS metric, query_id, query_sample, rows_examined AS value, calls, exec_time, temp_disk_tables\n",
    "    FROM rows_rank\n",
    "    WHERE rank <= {limit}\n",
    "    UNION ALL\n",
    "    SELECT 'Temp Disk Tables' AS metric, query_id, query_sample, temp_disk_tables AS value, calls, exec_time, rows_examined\n",
    "    FROM temp_tables_rank\n",
    "    WHERE rank <= {limit}\n",
    "    ORDER BY metric, value DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql(query, engine)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching top queries by multiple metrics: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Get top MySQL queries by multiple metrics\n",
    "multi_metric_df = get_top_by_multiple_metrics('mysql', time_window_minutes=60, limit=5)\n",
    "\n",
    "if not multi_metric_df.empty:\n",
    "    # Format for display\n",
    "    multi_metric_df['query_sample'] = multi_metric_df['query_sample'].apply(format_sample)\n",
    "    \n",
    "    # Convert exec_time to seconds if needed\n",
    "    exec_time_df = multi_metric_df[multi_metric_df['metric'] == 'Execution Time'].copy()\n",
    "    if not exec_time_df.empty and exec_time_df['value'].max() > 1000000:\n",
    "        # Values are likely in nanoseconds, convert to seconds\n",
    "        exec_time_df['value'] = exec_time_df['value'] / 1000000000\n",
    "        exec_time_df['exec_time'] = exec_time_df['exec_time'] / 1000000000\n",
    "        \n",
    "    # Create multi-metric comparison visualization\n",
    "    for metric in multi_metric_df['metric'].unique():\n",
    "        metric_df = multi_metric_df[multi_metric_df['metric'] == metric].copy()\n",
    "        \n",
    "        if metric == 'Execution Time' and not exec_time_df.empty:\n",
    "            metric_df = exec_time_df\n",
    "            title_suffix = \" (seconds)\"\n",
    "        else:\n",
    "            title_suffix = \"\"\n",
    "            \n",
    "        fig = px.bar(\n",
    "            metric_df,\n",
    "            x='query_id',\n",
    "            y='value',\n",
    "            hover_data=['query_sample', 'calls'],\n",
    "            title=f\"Top 5 Queries by {metric}{title_suffix}\",\n",
    "            labels={\n",
    "                'query_id': 'Query ID',\n",
    "                'value': metric,\n",
    "                'query_sample': 'Query',\n",
    "                'calls': 'Executions'\n",
    "            }\n",
    "        )\n",
    "        fig.update_layout(height=400)\n",
    "        fig.show()\n",
    "        \n",
    "        # Display table\n",
    "        display(metric_df[['query_sample', 'value', 'calls']].rename(\n",
    "            columns={\n",
    "                'query_sample': 'Query',\n",
    "                'value': metric,\n",
    "                'calls': 'Executions'\n",
    "            }\n",
    "        ))\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"No query data found for multiple metrics analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}